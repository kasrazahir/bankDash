{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import hashlib, binascii\n",
    "import numpy as np\n",
    "import time\n",
    "from shutil import copyfile\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles():\n",
    "    dlFiles = listdir(\"./download\")\n",
    "    try:\n",
    "        dlFiles.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    result = []\n",
    "    for file in dlFiles:\n",
    "        fileObject={\"name\":None, \"data\":None}\n",
    "        newData = pd.read_csv(\"./download/\"+file, header=None, names=newColumns)\n",
    "        fileObject[\"name\"] = file.split(\".\")[0]\n",
    "        fileObject[\"data\"] = newData\n",
    "        result.append(fileObject)\n",
    "    return result\n",
    "\n",
    "\n",
    "def getFile(file):\n",
    "    if file == \"data\":\n",
    "        return pd.read_csv(\"./processed/data.csv\", header=None, names=oldColumns,index_col=False)\n",
    "    elif file == \"processed\":\n",
    "        return pd.read_csv(\"./processed/processed.csv\",index_col=False)\n",
    "    elif file == \"maps\":\n",
    "        return pd.read_csv(\"./rules/1to1maps.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"subCategories\":\n",
    "        return pd.read_csv(\"./rules/categories.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"categories\":\n",
    "        return pd.read_csv(\"./rules/breakdown.csv\", header=None, names=['subCategory', 'category'])\n",
    "\n",
    "def saveDf(df, fileName, path, header):\n",
    "    miliTime = int(round(time.time() * 1000))\n",
    "    df.to_csv(f'./backup/{fileName}-{str(miliTime)}.csv', index=False, header=header)\n",
    "    df.to_csv(f'./{path}/{fileName}.csv', index=False, header=header)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash Data\n",
    "\n",
    "oldColumns=['date','item','debit','credit','subCategory','hash', 'account']\n",
    "addedColumns = ['date','item','debit','credit','hash','account']\n",
    "newColumns=['date','item','debit','credit','card']\n",
    "processedColumns=['item','category','subCategory','date','year','month','debit','credit','balance', 'account']\n",
    "\n",
    "def hashit(df):\n",
    "    hashs = []\n",
    "    for index, row in df.iterrows():\n",
    "        h = hashlib.new('ripemd160')\n",
    "        it = str(row['item'])\n",
    "        it2= ' '.join(re.findall(r\"[\\w']+\", it))\n",
    "        h.update(it2.encode())\n",
    "        h.update(str(row['credit']).encode())\n",
    "        h.update(str(row['debit']).encode())\n",
    "        h.update(str(row['date']).encode())\n",
    "        hashed =h.hexdigest()\n",
    "        hashs.append(hashed)\n",
    "    return hashs\n",
    "\n",
    "def testDf(df):\n",
    "    assert df.dtypes['debit'] == 'float64'\n",
    "    assert df.dtypes['credit'] == 'float64'\n",
    "\n",
    "def fixDf(df):\n",
    "    global oldColumns\n",
    "    df['subCategory'].fillna(\"\",inplace=True)\n",
    "    df=df[oldColumns]\n",
    "    return df\n",
    "\n",
    "def findNewItems(old, new, fileName):\n",
    "    new['hash']= hashit(new)\n",
    "    hashfound = []\n",
    "    for index, row in new.iterrows():\n",
    "        hashfound.append(row['hash'] in old['hash'].values)\n",
    "    new['hashfound']=hashfound\n",
    "    new.loc[new['hashfound'] == False, 'account'] = fileName;\n",
    "    newItems = new[new['hashfound'] == False]\n",
    "    return newItems\n",
    "\n",
    "def writeToJson(df):\n",
    "    gooddata = df\n",
    "    items = []\n",
    "    for index, row in gooddata.iterrows():\n",
    "        temp = {}\n",
    "        for key in row.keys():\n",
    "            temp[key] = row[key]\n",
    "        items.append(temp)\n",
    "    with open('analysis/js/data.json', 'w') as jsonFile:\n",
    "        json.dump(items, jsonFile)\n",
    "\n",
    "def listNewItems(files):\n",
    "    global oldColumns\n",
    "    old = getFile('data')\n",
    "    old['subCategory'].fillna(\"\",inplace=True)\n",
    "    old.fillna(value=0,inplace=True)\n",
    "    testDf(old)\n",
    "\n",
    "    combinedAll = old[oldColumns]\n",
    "    combinedNew = pd.DataFrame(columns=oldColumns)\n",
    "\n",
    "    for new in files:\n",
    "        newData = new['data']\n",
    "        newName = new['name']\n",
    "        newData.fillna(value=0,inplace=True)\n",
    "        testDf(newData)\n",
    "\n",
    "        newItems = findNewItems(combinedAll, newData, newName)\n",
    "        newToSave = newItems[addedColumns]\n",
    "\n",
    "        print(f\"{newName} - {len(newToSave)} new items found\")\n",
    "\n",
    "        combinedAll = pd.concat([combinedAll, newToSave])\n",
    "        combinedNew = pd.concat([combinedNew, newToSave])\n",
    "        \n",
    "    combinedNew = fixDf(combinedNew)\n",
    "    return combinedNew\n",
    "\n",
    "# Process Hashed Data\n",
    "def processData(newItems,doAll = False):\n",
    "    if doAll:\n",
    "        data = getFile('data')\n",
    "    else:\n",
    "        newItems.reset_index(inplace=True)\n",
    "        data = newItems.copy()\n",
    "\n",
    "    maps = getFile('maps')\n",
    "    subCategories = getFile('subCategories')\n",
    "    categories = getFile('categories')\n",
    "\n",
    "    categoryMap ={}\n",
    "    for i, row in categories.iterrows():\n",
    "        categoryMap[row['subCategory']] = row['category']\n",
    "\n",
    "    data.fillna(\"\", inplace=True)\n",
    "\n",
    "    subCatArray = []\n",
    "    # first mapping the 1to1 mappings\n",
    "    for i, row in data.iterrows():\n",
    "        if (row['subCategory'] != \"\"):\n",
    "            subCatArray.append(row['subCategory'])\n",
    "        else:\n",
    "            try:\n",
    "                index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "                subCategory = maps.loc[index]['subCategory']\n",
    "                subCatArray.append(subCategory)\n",
    "            except:\n",
    "                subCatArray.append(\"\")\n",
    "\n",
    "    # then mapping all the general categories\n",
    "    data['subCategory'] = subCatArray\n",
    "    subCatArray = pd.Series(subCatArray) \n",
    "\n",
    "\n",
    "    for i, categoryRow in subCategories.iloc[::-1].iterrows():\n",
    "        indo = ((data['item'].str.contains(categoryRow['item'])) & (data['subCategory']==\"\"))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['balance']=data['credit']-data['debit']\n",
    "\n",
    "    # finally taking care of special categories with logic\n",
    "    specialCategories = subCategories[subCategories['item'].str.contains('{{')]\n",
    "    for i, categoryRow in specialCategories.iterrows():\n",
    "        itemValuePair = categoryRow['item'].replace('}}', '').split('{{')\n",
    "        indo = (data['item'].str.contains(itemValuePair[0].rstrip()) & (data['balance']==(float(itemValuePair[1]))))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['subCategory'] = subCatArray\n",
    "    data['category'] = data['subCategory'].map(categoryMap)\n",
    "    data['year']= pd.to_datetime(data['date']).dt.year\n",
    "    data['month']= pd.to_datetime(data['date']).dt.month    \n",
    "    return data\n",
    "\n",
    "def runProcess(files):\n",
    "    newItems = listNewItems(files)\n",
    "    if(newItems['item'].count() > 0):\n",
    "        processedData = processData(newItems)   \n",
    "        dataWithoutCategory = (processedData[processedData['subCategory'] == \"\"])\n",
    "        if(len(dataWithoutCategory) == 0):\n",
    "            processedAlready = getFile('processed')\n",
    "            processedAll = pd.concat([processedData, processedAlready])\n",
    "            processedToSave = processedAll[processedColumns].sort_values(by='date', ascending=False)\n",
    "            saveDf(processedToSave, 'processed', 'processed', True)\n",
    "\n",
    "            dataAll = getFile('data')\n",
    "            combinedData = pd.concat([dataAll, newItems])\n",
    "            combinedData = combinedData[oldColumns]\n",
    "            saveDf(combinedData, 'data', 'processed', False)\n",
    "\n",
    "            writeToJson(processedToSave)\n",
    "\n",
    "            print(\"SAVED\")\n",
    "        else:\n",
    "            print('Found Gaps, NOT SAVED')\n",
    "            print(dataWithoutCategory[['item','date','balance']])\n",
    "    #       dataWithoutCategory[['item','date','balance']].to_csv('./processed/not_found.csv')\n",
    "    else:\n",
    "        print('no new items')\n",
    "        \n",
    "def resetToCurrentData():\n",
    "    processedData = processData(None, True)  \n",
    "    processedToSave = processedData[processedColumns].sort_values(by='date', ascending=False)\n",
    "    saveDf(processedToSave, 'processed', 'processed', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line - 0 new items found\n",
      "visa2 - 0 new items found\n",
      "no new items\n"
     ]
    }
   ],
   "source": [
    "  # files should have schema [{'name':string, 'data':pd.DataFrame}, ...]\n",
    "files = getFiles()\n",
    "runProcess(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "resetToCurrentData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/processed.csv'"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset data and process from backup\n",
    "from shutil import copyfile\n",
    "copyfile(\"./backup/data.csv\", \"./processed/data.csv\")\n",
    "copyfile(\"./backup/processed.csv\", \"./processed/processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HASHING UTILITIES\n",
    "# hash the data file\n",
    "# old = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# old['custom'].fillna(\"\",inplace=True)\n",
    "# old.fillna(value=0,inplace=True)\n",
    "# assert old.dtypes['debit'] == 'float64'\n",
    "# assert old.dtypes['credit'] == 'float64'\n",
    "# old['hash']= hashit(old)\n",
    "# old.to_csv(f'./processed/data.csv', index=False, header=False)\n",
    "\n",
    "# hash the other files\n",
    "# for file in dlFiles:\n",
    "#     old = pd.read_csv(\"./processed/cards/\"+file, header=None,names=oldColumns)\n",
    "#     old['custom'].fillna(\"\",inplace=True)\n",
    "#     old.fillna(value=0,inplace=True)\n",
    "#     assert old.dtypes['debit'] == 'float64'\n",
    "#     assert old.dtypes['credit'] == 'float64'\n",
    "#     old['hash']= hashit(old)\n",
    "#     old.to_csv(f'./processed/{file.split(\".\")[0]}.{file.split(\".\")[1]}', index=False, header=False)\n",
    "\n",
    "# # populate account field in data\n",
    "# oldColumns=['date','item','debit','credit','custom','hash', 'account']\n",
    "\n",
    "# data = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for file in dlFiles:\n",
    "#     new = pd.read_csv(\"./processed/\"+file, header=None,names=oldColumns)\n",
    "#     for index, row in new.iterrows():\n",
    "#         if(row['hash'] in data['hash'].values):\n",
    "#             data.loc[data['hash'] == row['hash'],'account']=file.split(\".\")[0]\n",
    "            \n",
    "# data.to_csv(f'./processed/data.csv', index=False, header=False) \n",
    "\n",
    "\n",
    "# # check for duplicate fields\n",
    "\n",
    "# data2 = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for index, row in data2.iterrows():\n",
    "#     if(data2[data2['hash']==row['hash']].count()['hash']>2):\n",
    "#         print(row['hash'], row['item'], row['debit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PROCESS BENCH MARKING\n",
    "# # TESTING TWO FILES\n",
    "# old = pd.read_csv(\"./processed/output-old.csv\", index_col=False)\n",
    "# new = pd.read_csv(\"./processed/processed.csv\", index_col=False)\n",
    "# old.sort_values(by=['date','debit'], inplace=True)\n",
    "# new.sort_values(by=['date','debit'], inplace=True)\n",
    "# old.reset_index( drop=True,inplace=True)\n",
    "# new.reset_index(drop=True,inplace=True)\n",
    "# ineq = []\n",
    "# for i, row in old.iterrows():\n",
    "#     if (row['subCategory'] != new.loc[i]['subCategory']):\n",
    "#         if(row['item']==\"TIGERDIRECT.CA MARKHAM, ON\"):\n",
    "#             pass\n",
    "#         elif('WINNERSHOMESENSE' in row['item']):\n",
    "#             pass\n",
    "#         else:\n",
    "#             pass\n",
    "# #             print(f\"{row['subCategory']}, {new.loc[i]['subCategory']} - {row['balance']} {row['item']}{new.loc[i]['item']}\")\n",
    "            \n",
    "\n",
    "# y = new.groupby('subCategory').sum()['balance']\n",
    "# x = old.groupby('subCategory').sum()['balance']\n",
    "# for i,value in enumerate(x):\n",
    "#     if(value != y[i]):\n",
    "#         print(value, y[i])\n",
    "\n",
    "# # one to one map benchmarking\n",
    "# def try1():    \n",
    "#     def check1to1(x):\n",
    "#         try:\n",
    "#             index = pd.Index(maps['item']).get_loc(x.rstrip())   \n",
    "#             return maps.loc[index]['subCategory']\n",
    "#         except:\n",
    "#             return None\n",
    "\n",
    "#     data['subCategory'] = data['subCategory'].combine_first(data['item'].apply(check1to1))\n",
    "    \n",
    "# def try2():\n",
    "#     data.fillna(\"\", inplace=True)\n",
    "#     subCatArray = []\n",
    "#     # first mapping the 1to1 mappings\n",
    "#     for i, row in data.iterrows():\n",
    "#         if (row['subCategory'] != \"\"):\n",
    "#             subCatArray.append(row['subCategory'])\n",
    "#         else:\n",
    "#             try:\n",
    "#                 index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "#                 subCategory = maps.loc[index]['subCategory']\n",
    "#                 subCatArray.append(subCategory)\n",
    "#             except:\n",
    "#                 subCatArray.append(\"\")\n",
    "#     data['subCategory'] = subCatArray\n",
    "# %timeit try1()\n",
    "# %timeit try2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
